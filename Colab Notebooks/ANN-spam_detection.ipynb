{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+b6INQ5IM/smDxlVlNN+8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Sample Spam Dataset (Replace with your own dataset if available)\n","data = {\n","    \"text\": [\n","        \"Win a free iPhone now!\", \"Congrats! You won a lottery!\", \"Urgent: Claim your prize today!\",\n","        \"Hello, how are you?\", \"Let's have a meeting tomorrow.\", \"Are we still on for lunch?\",\n","        \"Limited time offer! Click here!\", \"Cheap loans available!\", \"Buy 1 get 1 free today!\",\n","        \"Good morning, have a great day!\"\n","    ],\n","    \"label\": [\"spam\", \"spam\", \"spam\", \"ham\", \"ham\", \"ham\", \"spam\", \"spam\", \"spam\", \"ham\"]\n","}\n","\n","# Convert to DataFrame\n","df = pd.DataFrame(data)\n","\n","# Encode Labels (Spam = 1, Ham = 0)\n","encoder = LabelEncoder()\n","df[\"label\"] = encoder.fit_transform(df[\"label\"])\n","\n","# Splitting into Train and Test Sets\n","X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)\n","\n","# Tokenizing Text Data\n","max_words = 1000  # Vocabulary size\n","max_len = 20  # Maximum sequence length\n","\n","tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len, padding=\"post\")\n","X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len, padding=\"post\")\n","\n","# Building the ANN Model\n","model = Sequential([\n","    Embedding(input_dim=max_words, output_dim=16, input_length=max_len),\n","    GlobalAveragePooling1D(),\n","    Dense(16, activation=\"relu\"),\n","    Dense(1, activation=\"sigmoid\")  # Output layer for binary classification\n","])\n","\n","# Compile Model\n","model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","\n","# Train the Model\n","model.fit(X_train_seq, y_train, epochs=10, validation_data=(X_test_seq, y_test), verbose=2)\n","\n","# Evaluate the Model\n","loss, accuracy = model.evaluate(X_test_seq, y_test)\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","\n","# Making Predictions\n","sample_texts = [\"Congratulations, you won a free gift!\", \"Let's meet for coffee.\"]\n","sample_seq = pad_sequences(tokenizer.texts_to_sequences(sample_texts), maxlen=max_len, padding=\"post\")\n","\n","predictions = model.predict(sample_seq)\n","pred_labels = [\"Spam\" if p > 0.5 else \"Ham\" for p in predictions]\n","\n","# Display Predictions\n","for text, label in zip(sample_texts, pred_labels):\n","    print(f\"Message: {text} â†’ Prediction: {label}\")\n","\n"],"metadata":{"id":"I6_eB1Etz1E0"},"execution_count":null,"outputs":[]}]}